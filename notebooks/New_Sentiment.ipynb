{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-01T13:46:38.773325Z",
     "iopub.status.busy": "2025-04-01T13:46:38.773112Z",
     "iopub.status.idle": "2025-04-01T13:46:38.777154Z",
     "shell.execute_reply": "2025-04-01T13:46:38.776237Z",
     "shell.execute_reply.started": "2025-04-01T13:46:38.773305Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\smoha\\OneDrive\\Desktop\\Stocks\\newVenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load Data and Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T13:46:38.792174Z",
     "iopub.status.busy": "2025-04-01T13:46:38.791955Z",
     "iopub.status.idle": "2025-04-01T13:46:38.807883Z",
     "shell.execute_reply": "2025-04-01T13:46:38.807095Z",
     "shell.execute_reply.started": "2025-04-01T13:46:38.792153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/interim/News1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T13:46:41.054635Z",
     "iopub.status.busy": "2025-04-01T13:46:41.054320Z",
     "iopub.status.idle": "2025-04-01T13:46:41.067481Z",
     "shell.execute_reply": "2025-04-01T13:46:41.066529Z",
     "shell.execute_reply.started": "2025-04-01T13:46:41.054610Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4555 entries, 0 to 4554\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   publish_date       4555 non-null   object\n",
      " 1   headline_category  4555 non-null   object\n",
      " 2   headline_text      4555 non-null   object\n",
      " 3   main_category      4555 non-null   object\n",
      " 4   sub_category       3363 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 178.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are going to use FinBert model in order to find sentiments for the news headlines\n",
    "- We will collect all three scores - positive, negative and neutral along with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T13:46:43.458607Z",
     "iopub.status.busy": "2025-04-01T13:46:43.458289Z",
     "iopub.status.idle": "2025-04-01T13:46:45.562367Z",
     "shell.execute_reply": "2025-04-01T13:46:45.561448Z",
     "shell.execute_reply.started": "2025-04-01T13:46:43.458581Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\smoha\\OneDrive\\Desktop\\Stocks\\newVenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\smoha\\OneDrive\\Desktop\\Stocks\\newVenv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at yiyanghkust/finbert-tone.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "# for pytorch\n",
    "# model = pipeline(task = \"text-classification\",  model=\"ProsusAI/finbert\", return_all_scores = True)\n",
    "\n",
    "# for tensorflow\n",
    "model = pipeline(task = \"text-classification\",  model=\"yiyanghkust/finbert-tone\", tokenizer=\"yiyanghkust/finbert-tone\", framework=\"tf\", return_all_scores = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T13:46:54.323512Z",
     "iopub.status.busy": "2025-04-01T13:46:54.323196Z",
     "iopub.status.idle": "2025-04-01T13:46:54.327196Z",
     "shell.execute_reply": "2025-04-01T13:46:54.326252Z",
     "shell.execute_reply.started": "2025-04-01T13:46:54.323488Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df2 = df.sample(frac=1, random_state=42).reset_index(drop=True) # if you want to shuffle\n",
    "df2 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T13:46:55.243789Z",
     "iopub.status.busy": "2025-04-01T13:46:55.243404Z",
     "iopub.status.idle": "2025-04-01T13:46:55.249508Z",
     "shell.execute_reply": "2025-04-01T13:46:55.248590Z",
     "shell.execute_reply.started": "2025-04-01T13:46:55.243757Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for storing the predicted values\n",
    "sentiment = [np.nan for i in range(len(df2))]\n",
    "pos_score = [np.nan for i in range(len(df2))]\n",
    "neg_score = [np.nan for i in range(len(df2))]\n",
    "neu_score = [np.nan for i in range(len(df2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T13:46:56.504055Z",
     "iopub.status.busy": "2025-04-01T13:46:56.503725Z",
     "iopub.status.idle": "2025-04-01T13:46:56.508571Z",
     "shell.execute_reply": "2025-04-01T13:46:56.507575Z",
     "shell.execute_reply.started": "2025-04-01T13:46:56.504030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment_features(text):\n",
    "    \"\"\"\n",
    "        Get sentiment scores and labels using FinBERT\n",
    "    \"\"\"\n",
    "    result = model.predict(text)[0]\n",
    "    \n",
    "    # store the values in dict as label : score\n",
    "    scores = {res['label']: res['score'] for res in result}\n",
    "\n",
    "    # Get highest score label as output label for this text\n",
    "    label = max(scores, key=scores.get)\n",
    "\n",
    "    # return positive, negative, neutral and label    \n",
    "    return scores[\"Positive\"], scores[\"Negative\"], scores[\"Neutral\"], label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T13:46:58.240275Z",
     "iopub.status.busy": "2025-04-01T13:46:58.239976Z",
     "iopub.status.idle": "2025-04-01T13:46:58.243716Z",
     "shell.execute_reply": "2025-04-01T13:46:58.242921Z",
     "shell.execute_reply.started": "2025-04-01T13:46:58.240254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for predicting labels only for some of the records\n",
    "\n",
    "\n",
    "# batch_size = 6\n",
    "\n",
    "# for i in range(0, len(df2), batch_size):\n",
    "#     sample = df2.iloc[i]\n",
    "#     prediction = get_sentiment_features(sample.headline_text)\n",
    "#     sentiment[i] = prediction[3]\n",
    "#     pos_score[i] = prediction[0]\n",
    "#     neg_score[i] = prediction[1]\n",
    "#     neu_score[i] = prediction[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T13:46:59.466015Z",
     "iopub.status.busy": "2025-04-01T13:46:59.465687Z",
     "iopub.status.idle": "2025-04-01T14:00:17.741586Z",
     "shell.execute_reply": "2025-04-01T14:00:17.740807Z",
     "shell.execute_reply.started": "2025-04-01T13:46:59.465988Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for each record find prediction\n",
    "\n",
    "for i in range(0, len(df2)):\n",
    "    sample = df2.iloc[i]\n",
    "    prediction = get_sentiment_features(sample.headline_text)\n",
    "    sentiment[i] = prediction[3]\n",
    "    pos_score[i] = prediction[0]\n",
    "    neg_score[i] = prediction[1]\n",
    "    neu_score[i] = prediction[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:00:17.742875Z",
     "iopub.status.busy": "2025-04-01T14:00:17.742633Z",
     "iopub.status.idle": "2025-04-01T14:00:17.751031Z",
     "shell.execute_reply": "2025-04-01T14:00:17.750092Z",
     "shell.execute_reply.started": "2025-04-01T14:00:17.742855Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# store the predicted values in df\n",
    "\n",
    "df2['sentiment'] = sentiment\n",
    "df2['pos_score'] = pos_score\n",
    "df2['neg_score'] = neg_score\n",
    "df2['neu_score'] = neu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:00:17.773375Z",
     "iopub.status.busy": "2025-04-01T14:00:17.773181Z",
     "iopub.status.idle": "2025-04-01T14:00:17.787013Z",
     "shell.execute_reply": "2025-04-01T14:00:17.786396Z",
     "shell.execute_reply.started": "2025-04-01T14:00:17.773358Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# encoding the categories\n",
    "\n",
    "label_mapping = {\"positive\": 1, \"neutral\": 0, \"negative\": -1}\n",
    "df2[\"sentiment_label_num\"] = df2[\"sentiment\"].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:00:17.813483Z",
     "iopub.status.busy": "2025-04-01T14:00:17.813253Z",
     "iopub.status.idle": "2025-04-01T14:00:17.823410Z",
     "shell.execute_reply": "2025-04-01T14:00:17.822494Z",
     "shell.execute_reply.started": "2025-04-01T14:00:17.813463Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4555 entries, 0 to 4554\n",
      "Data columns (total 10 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   publish_date         4555 non-null   object \n",
      " 1   headline_category    4555 non-null   object \n",
      " 2   headline_text        4555 non-null   object \n",
      " 3   main_category        4555 non-null   object \n",
      " 4   sub_category         3363 non-null   object \n",
      " 5   sentiment            4555 non-null   object \n",
      " 6   pos_score            4555 non-null   float64\n",
      " 7   neg_score            4555 non-null   float64\n",
      " 8   neu_score            4555 non-null   float64\n",
      " 9   sentiment_label_num  0 non-null      float64\n",
      "dtypes: float64(4), object(6)\n",
      "memory usage: 356.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:08:16.315213Z",
     "iopub.status.busy": "2025-04-01T14:08:16.314850Z",
     "iopub.status.idle": "2025-04-01T14:08:16.356228Z",
     "shell.execute_reply": "2025-04-01T14:08:16.355294Z",
     "shell.execute_reply.started": "2025-04-01T14:08:16.315187Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df2.to_csv(\"../data/interim/News2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below code was return for semi-supervised learning approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "sentiment_encoded = encoder.fit_transform(df2[[\"sentiment\"]].fillna(\"unknown\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sentiment_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sentiment_encoded_df = pd.DataFrame(sentiment_encoded, columns=encoder.get_feature_names_out([\"sentiment\"]))\n",
    "df3 = pd.concat([df2, sentiment_encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "\n",
    "# Load the FinBERT model and tokenizer using TensorFlow\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "model = TFAutoModel.from_pretrained(\"yiyanghkust/finbert-tone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    \"\"\"Convert text into a numerical vector using FinBERT in TensorFlow\"\"\"\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True)\n",
    "    \n",
    "    # Run the model and get the hidden state embeddings\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # The embeddings are in the last_hidden_state attribute (shape: [batch_size, seq_length, hidden_size])\n",
    "    # We take the mean of all token embeddings (mean pooling)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    embedding = tf.reduce_mean(last_hidden_state, axis=1)  # Mean pooling over tokens\n",
    "    \n",
    "    # Convert tensor to numpy array and remove extra dimensions\n",
    "    return embedding.numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df2[\"embedding\"] = df2[\"headline_text\"].apply(lambda x: get_embedding(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedding = np.vstack(df2.embedding.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df2.fillna({\"pos_score\": 0, \"neg_score\": 0, \"neu_score\": 0, \"sentiment_label_num\": 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sentiment_features = df2[[\"pos_score\", \"neg_score\", \"neu_score\", \"sentiment_label_num\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = np.hstack((embedding, sentiment_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# X = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_clusters = 3  # Adjust based on dataset\n",
    "# kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "dbscan = DBSCAN(min_samples=5)\n",
    "# clusters = kmeans.fit_predict(X)\n",
    "clusters = dbscan.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df2[\"cluster\"] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df2.groupby(\"cluster\")[\"sentiment\"].agg(lambda x: x.mode()[0] if not x.isna().all() else \"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df2.cluster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cluster_sentiment_map = df2.groupby(\"cluster\")[\"sentiment\"].agg(lambda x: x.mode()[0] if not x.isna().all() else \"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Assign labels to previously unlabeled data\n",
    "df2[\"final_sentiment_label\"] = df2[\"cluster\"].map(cluster_sentiment_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6971864,
     "sourceId": 11171458,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "newVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
